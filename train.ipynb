import torch

print(torch.cuda.is_available())  # True
print(torch.cuda.get_device_name(0))  # "NVIDIA GeForce RTX 4070 SUPER"
from datasets import load_dataset

dataset = load_dataset("jeanlee/kmhas_korean_hate_speech")

print(dataset)
print(dataset['train'][0])

# 다중 레이블 필드 지정
label_columns = [
    'hate', 'offensive', 'gender', 'age', 'religion', 'politics',
    'appearance', 'region', 'sexual_orientation'
]


# 다중 레이블 벡터로 묶기
def format_labels(example):
    example["labels"] = example["label"]  # 그냥 그대로 복사
    return example


encoded_dataset = dataset.map(format_labels)

NUM_CLASSES = 9  # hate~sexual_orientation


def to_multihot(example):
    multi_hot = [0] * NUM_CLASSES
    for idx in example["labels"]:
        multi_hot[idx] = 1
    example["labels"] = multi_hot
    return example


encoded_dataset = encoded_dataset.map(to_multihot)

from transformers import AutoTokenizer

model_name = "beomi/KcELECTRA-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)


def tokenize_function(example):
    return tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )


tokenized_dataset = encoded_dataset.map(tokenize_function, batched=True)

tokenized_dataset = tokenized_dataset.remove_columns(['text', 'label'])
tokenized_dataset.set_format("torch")

print(tokenized_dataset["train"][0])

from transformers import ElectraForSequenceClassification
import torch

model = ElectraForSequenceClassification.from_pretrained(
    "beomi/KcELECTRA-base",
    num_labels=9,
    problem_type="multi_label_classification"
)

model.to("cuda")  # ✅ 수동으로 GPU에 올림

import numpy as np
from sklearn.metrics import f1_score, accuracy_score


def compute_metrics(pred):
    logits, labels = pred
    probs = torch.sigmoid(torch.tensor(logits)).numpy()  # 확률화
    preds = (probs >= 0.5).astype(int)  # 0.5 기준 이진화
    labels = labels.astype(int)

    return {
        'accuracy': accuracy_score(labels, preds),
        'micro_f1': f1_score(labels, preds, average='micro'),
        'macro_f1': f1_score(labels, preds, average='macro')
    }


import transformers

print(transformers.__version__)
# ✅ 4.40.x 이상이면 OK

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="micro_f1",
    save_total_limit=1,
    report_to="none"
)

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="pt")

# ✅ Trainer 정의 시 이걸 추가!
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

import numpy as np
from sklearn.metrics import f1_score, accuracy_score


def compute_metrics(pred):
    logits, labels = pred
    probs = torch.sigmoid(torch.tensor(logits)).numpy()
    preds = (probs >= 0.5).astype(int)
    labels = labels.astype(int)
    return {
        'accuracy': accuracy_score(labels, preds),
        'micro_f1': f1_score(labels, preds, average='micro'),
        'macro_f1': f1_score(labels, preds, average='macro')
    }


def to_multihot(example):
    multi_hot = [0.0] * 9  # float으로 초기화
    for idx in example["label"]:
        multi_hot[idx] = 1.0
    example["labels"] = multi_hot
    return example


encoded_dataset = dataset.map(to_multihot)

tokenized_dataset = encoded_dataset.map(tokenize_function, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(['text', 'label'])
tokenized_dataset.set_format("torch")

# 토크나이징 완료 후
tokenized_dataset = encoded_dataset.map(tokenize_function, batched=True)


# 👇 여기서 float32 변환 추가!
def cast_label_to_float(example):
    example["labels"] = [float(x) for x in example["labels"]]
    return example


tokenized_dataset = tokenized_dataset.map(cast_label_to_float)

# 그 다음 나머지 그대로 유지
tokenized_dataset = tokenized_dataset.remove_columns(['text', 'label'])
tokenized_dataset.set_format("torch")


def cast_label_to_float(example):
    example["labels"] = torch.tensor(example["labels"], dtype=torch.float32)
    return example


tokenized_dataset = tokenized_dataset.map(cast_label_to_float)
tokenized_dataset.set_format("torch")


# float32로 명확히 Tensor화
def cast_label_to_float(example):
    example["labels"] = torch.tensor(example["labels"], dtype=torch.float32)
    return example


tokenized_dataset = tokenized_dataset.map(cast_label_to_float)

# ⚠️ dtype 없이 그냥 columns만 지정!
tokenized_dataset.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "token_type_ids", "labels"]
)

print(tokenized_dataset["train"][0]["labels"].dtype)


# ✅ torch.float32 출력되면 성공

def predict_text(text):
    # 입력 텍스트 토크나이징
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # 모델 추론
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.sigmoid(logits).cpu().numpy()[0]  # 확률 값

    # 예측 결과 해석
    threshold = 0.5
    predicted = (probs >= threshold).astype(int)

    # 라벨 리스트
    label_columns = [
        "hate", "offensive", "gender", "age", "religion",
        "politics", "appearance", "region", "sexual_orientation"
    ]
    result = {label: float(prob) for label, prob in zip(label_columns, probs)}
    activated = [label for label, val in zip(label_columns, predicted) if val == 1]

    return result, activated


trainer.train()

sentence = "저 사람은 왜 저럴까?"
result, activated_labels = predict_text(sentence)

print("🔍 예측 확률:")
for label, score in result.items():
    print(f"{label:>20}: {score:.4f}")

print("\n✅ 해당 문장에서 탐지된 라벨:")
print(activated_labels)

from datasets import load_dataset

ds = load_dataset("ucberkeley-dlab/measuring-hate-speech")

print(ds)
print(ds["train"][0])

from datasets import load_dataset
from transformers import (
    AutoTokenizer, ElectraForSequenceClassification,
    TrainingArguments, Trainer, EarlyStoppingCallback
)
import torch
from sklearn.metrics import accuracy_score, f1_score

# ✅ 1. 데이터 로드 및 전처리
dataset = load_dataset("ucberkeley-dlab/measuring-hate-speech")

target_labels = [
    'respect', 'insult', 'humiliate', 'dehumanize', 'violence',
    'genocide', 'attack_defend', 'hatespeech', 'hate_speech_score'
]


def preprocess(example):
    labels = [float(example[label]) for label in target_labels]
    return {
        "text": example["text"],
        "labels": torch.tensor(labels, dtype=torch.float)
    }


dataset = dataset["train"].map(preprocess)
dataset = dataset.train_test_split(test_size=0.2)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# ✅ 2. 토크나이징
tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base")


def tokenize(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=128)


train_dataset = train_dataset.map(tokenize, batched=True)
eval_dataset = eval_dataset.map(tokenize, batched=True)

# ✅ 3. 학습용 포맷 설정
columns_to_keep = ['input_ids', 'attention_mask', 'labels']
train_dataset.set_format(type="torch", columns=columns_to_keep)
eval_dataset.set_format(type="torch", columns=columns_to_keep)

# ✅ 4. 모델 정의
model = ElectraForSequenceClassification.from_pretrained(
    "beomi/KcELECTRA-base",
    num_labels=len(target_labels),
    problem_type="multi_label_classification"
)


# ✅ 5. 평가지표 정의
def compute_metrics(pred):
    import numpy as np
    from sklearn.metrics import accuracy_score, f1_score

    logits, labels = pred
    probs = torch.sigmoid(torch.tensor(logits)).numpy()

    # ✅ 이진화된 예측값
    preds = (probs >= 0.5).astype(int)

    # ✅ float -> int 변환
    labels = np.array(labels)
    if labels.dtype != int:
        labels = (labels >= 0.5).astype(int)

    return {
        "accuracy": accuracy_score(labels, preds),
        "micro_f1": f1_score(labels, preds, average="micro", zero_division=0),
        "macro_f1": f1_score(labels, preds, average="macro", zero_division=0)
    }


# ✅ 6. 학습 파라미터 정의
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=10,  # 충분히 큰 수 지정
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="micro_f1",
    save_total_limit=1,
    report_to="none"
)

# ✅ 7. Trainer 정의 및 학습 실행 (EarlyStopping 추가)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # 2 epoch 동안 개선 없으면 중단
)

trainer.train()
